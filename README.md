# Projeto NoSQL IoT - Engenharia de Dados com Apache Airflow + Docker

Este projeto simula um caso real da Petrobras: sensores IoT monitoram bombas e compressores em plataformas offshore. Esses dados chegam em JSON, s√£o tratados com Python e orquestrados com Apache Airflow, e salvos como Parquet para an√°lise.

---

## O que voc√™ precisa ter instalado?

### 1. [Python 3.12+](https://www.python.org/downloads/)

### 2. [Git](https://git-scm.com/)

### 3. [Docker Desktop](https://www.docker.com/products/docker-desktop/)

### 4. [Visual Studio Code (VSCode)](https://code.visualstudio.com/) (opcional, mas recomendado)

---

## Clonar o projeto

Abra o terminal e digite:

```bash
git clone https://github.com/SEU_USUARIO/projeto-nosql-iot.git
cd projeto-nosql-iot
```

---

## Criar ambiente virtual e instalar as depend√™ncias

```bash
python -m venv .venv
.venv\Scripts\activate   # Windows
# source .venv/bin/activate   # Linux/macOS

pip install -r airflow/requirements.txt
```

---

## Subir o Apache Airflow com Docker

### 1. Rode os containers:

```bash
docker-compose up -d
```

### 2. Acesse o Airflow no navegador:

```
http://localhost:8080
```

- **Usu√°rio**: `admin`
- **Senha**: `admin`

---

## Estrutura de Pastas

```bash
projeto-nosql-iot/
‚îú‚îÄ‚îÄ .github/
‚îÇ   ‚îî‚îÄ‚îÄ workflows/
‚îÇ       ‚îî‚îÄ‚îÄ python-pipeline.yml         # CI/CD com GitHub Actions
‚îú‚îÄ‚îÄ airflow/
‚îÇ   ‚îú‚îÄ‚îÄ dags/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dag_pipeline.py             # DAG do Airflow
‚îÇ   ‚îú‚îÄ‚îÄ logs/                           # Logs do Airflow
‚îÇ   ‚îú‚îÄ‚îÄ plugins/                        # Plugins (se necess√°rio)
‚îÇ   ‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ gerar_dados_json.py         # Script para gerar JSON simulado
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt                # Requisitos do Airflow
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ lake/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ sensores_lake.parquet       # Arquivo final
‚îÇ   ‚îú‚îÄ‚îÄ processed/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sensores_extraidos.csv
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sensores_extraidos.parquet
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sensores_transformados.csv
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ sensores_transformados.parquet
‚îÇ   ‚îî‚îÄ‚îÄ raw/
‚îÇ       ‚îî‚îÄ‚îÄ sensores_simulados.json     # Dados brutos simulados
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îî‚îÄ‚îÄ exploracao_inicial.ipynb        # An√°lises explorat√≥rias
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ carga.py                        # Carrega dados finais
‚îÇ   ‚îú‚îÄ‚îÄ extracao.py                     # Extrai dados do JSON
‚îÇ   ‚îú‚îÄ‚îÄ pipeline.py                     # Executa pipeline completo
‚îÇ   ‚îî‚îÄ‚îÄ transformacao.py                # Transforma os dados extra√≠dos
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îî‚îÄ‚îÄ test_transformacao.py           # Testes unit√°rios da transforma√ß√£o
‚îú‚îÄ‚îÄ docker-compose.yml                 # Configura√ß√£o do Docker
‚îî‚îÄ‚îÄ README.md                          # Documenta√ß√£o principal
```

---

## üó∫Ô∏è Fluxo do Pipeline de Dados IoT

A imagem abaixo representa todo o fluxo do projeto, desde a simula√ß√£o dos dados at√© a an√°lise final dos sensores:

![Fluxo do Pipeline](./img/fluxo.png)

---
## Executar o pipeline (via Airflow)

1. Acesse o navegador em `http://localhost:8080`
2. Ative a DAG `pipeline_iot_nosql`
3. Clique no bot√£o ‚ñ∂Ô∏è para rodar manualmente
4. Veja o gr√°fico com as etapas:

---

## Executar manualmente pelo terminal (sem Airflow)

```bash
# Rodar o pipeline completo com Python:
python src/pipeline.py
```

---

## Rodar os testes

```bash
# Windows
$env:PYTHONPATH="." ; pytest tests/

# Linux/macOS
PYTHONPATH=. pytest tests/
```

---

## Como o Projeto Foi Constru√≠do (Etapas)

### 1. Planejamento da Arquitetura
- Definimos uma estrutura baseada em extra√ß√£o, transforma√ß√£o e carga (ETL/ELT).
- Escolhemos o Apache Airflow para orquestrar e Docker para facilitar a implanta√ß√£o.

### 2. Simula√ß√£o de Dados IoT
- Criado um script em `airflow/scripts/gerar_dados_json.py` que simula sensores de bombas e compressores em plataformas offshore.
- Esse script gera um arquivo JSON com 50 registros.

### 3. Constru√ß√£o do Pipeline
- **Extra√ß√£o** (`src/extracao.py`): L√™ o JSON simulado e converte para DataFrame.
- **Transforma√ß√£o** (`src/transformacao.py`): Limpeza, convers√£o de tipos e normaliza√ß√£o.
- **Carga** (`src/carga.py`): Salva os dados em formato Parquet no diret√≥rio `data/lake/`.
- **Orquestra√ß√£o com DAG Airflow**: Criamos a DAG `dag_pipeline.py` que chama os scripts em ordem.

### 4. Cont√™ineriza√ß√£o com Docker
- Criado `docker-compose.yml` com os servi√ßos: PostgreSQL, Webserver, Scheduler e Init para Airflow.
- Montadas as pastas `./src`, `./data`, `./airflow` como volumes no cont√™iner.

### 5. Valida√ß√£o com Testes
- Criado `tests/test_transformacao.py` usando `pytest` para validar a transforma√ß√£o dos dados.

### 6. An√°lise com Jupyter Notebook
- Criado `notebooks/exploracao_inicial.ipynb`.
- Leitura do Parquet final e gera√ß√£o de gr√°ficos com `matplotlib` e `seaborn`.

---

## An√°lise que Simula o desempenho dos Sensores IoT que Monitoram Bombas e Compressores em Plataformas Offshore - Petrobras

### Contexto
Essa an√°lise simula a atua√ß√£o de sensores inteligentes em plataformas offshore, monitorando bombas e compressores. A an√°lise tem como base os dados transformados e salvos no formato Parquet (`data/lake/sensores_lake.parquet`) ap√≥s o pipeline.

Com os dados estruturados, utilizamos Python + Pandas para visualizar e entender os seguintes KPIs:
- Distribui√ß√£o por tipo de sensor
- M√©dias e desvios dos valores capturados
- Evolu√ß√£o dos sensores ao longo do tempo
- Detec√ß√£o de anomalias ou valores fora do padr√£o

### Exemplos de An√°lises Geradas
- Gr√°fico de linha para temperatura ao longo do tempo
- Boxplot comparando valores de sensores
- Histogramas por tipo de sensor
- Contagem de sensores por tipo

### Ferramentas
- `pandas`
- `matplotlib`
- `seaborn`

> Voc√™ pode executar a an√°lise abrindo o Jupyter Notebook em `notebooks/exploracao_inicial.ipynb`

---

## Prints do Funcionamento

### Docker Desktop com os containers ativos:
![docker](./img/docker_containers.png)

### DAG executada com sucesso no Airflow:
![airflow](./img/airflow_dag_pipeline.png)

---

## Poss√≠veis Evolu√ß√µes

- Integra√ß√£o com MongoDB Atlas real
- Deploy na nuvem (S3 / Azure Blob)
- Visualiza√ß√£o com Streamlit ou Power BI
- Kafka para streaming de dados

---

## Autor

**Adriano Vilela**\
Engenheiro de Dados em forma√ß√£o | Pythonista em constru√ß√£o\
[LinkedIn](https://linkedin.com/in/adrianogvs) ‚Ä¢ [GitHub](https://github.com/Adrianogvs)

---

Pronto! Agora √© s√≥ apertar o play na DAG e ver a m√°gia acontecer!