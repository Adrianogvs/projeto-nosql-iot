# Projeto NoSQL IoT - Engenharia de Dados com Apache Airflow + Docker
![CI](https://github.com/Adrianogvs/projeto-nosql-iot/actions/workflows/python-pipeline.yml/badge.svg)

Este projeto simula um caso real da Petrobras: sensores IoT monitoram bombas e compressores em plataformas offshore. Esses dados chegam em JSON, s√£o tratados com Python e orquestrados com Apache Airflow, e salvos como Parquet para an√°lise.

---

## üìÇ O que voc√™ precisa ter instalado?

### 1. [Python 3.12+](https://www.python.org/downloads/)
### 2. [Git](https://git-scm.com/)
### 3. [Docker Desktop](https://www.docker.com/products/docker-desktop/)
### 4. [Visual Studio Code (VSCode)](https://code.visualstudio.com/) (opcional, mas recomendado)

---

## üîÑ Clonar o projeto

Abra o terminal e digite:

```bash
git clone https://github.com/Adrianogvs/projeto-nosql-iot.git
cd projeto-nosql-iot
```

---

## üìä Criar ambiente virtual e instalar as depend√™ncias

```bash
python -m venv .venv
.venv\Scripts\activate   # Windows
# source .venv/bin/activate   # Linux/macOS

pip install -r airflow/requirements.txt
```

---

## üöÄ Subir o Apache Airflow com Docker

### 1. Rode os containers:
```bash
docker-compose up -d
```

### 2. Acesse o Airflow no navegador:
```
http://localhost:8080
```
- **Usu√°rio**: `admin`
- **Senha**: `admin`

---

## üìÅ Estrutura de Pastas

```bash
projeto-nosql-iot/
‚îú‚îÄ‚îÄ .github/
‚îÇ   ‚îî‚îÄ‚îÄ workflows/
‚îÇ       ‚îî‚îÄ‚îÄ python-pipeline.yml         # CI/CD com GitHub Actions
‚îú‚îÄ‚îÄ airflow/
‚îÇ   ‚îú‚îÄ‚îÄ dags/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dag_pipeline.py             # DAG do Airflow
‚îÇ   ‚îú‚îÄ‚îÄ logs/                           # Logs do Airflow
‚îÇ   ‚îú‚îÄ‚îÄ plugins/                        # Plugins (se necess√°rio)
‚îÇ   ‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ gerar_dados_json.py         # Script para gerar JSON simulado
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt                # Requisitos do Airflow
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ lake/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ sensores_lake.parquet       # Arquivo final
‚îÇ   ‚îú‚îÄ‚îÄ processed/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sensores_extraidos.csv
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sensores_extraidos.parquet
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sensores_transformados.csv
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ sensores_transformados.parquet
‚îÇ   ‚îî‚îÄ‚îÄ raw/
‚îÇ       ‚îî‚îÄ‚îÄ sensores_simulados.json     # Dados brutos simulados
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îî‚îÄ‚îÄ exploracao_inicial.ipynb        # An√°lises explorat√≥rias
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ carga.py                        # Carrega dados finais
‚îÇ   ‚îú‚îÄ‚îÄ extracao.py                     # Extrai dados do JSON
‚îÇ   ‚îú‚îÄ‚îÄ pipeline.py                     # Executa pipeline completo
‚îÇ   ‚îî‚îÄ‚îÄ transformacao.py                # Transforma os dados extra√≠dos
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îî‚îÄ‚îÄ test_transformacao.py           # Testes unit√°rios da transforma√ß√£o
‚îú‚îÄ‚îÄ docker-compose.yml                 # Configura√ß√£o do Docker
‚îî‚îÄ‚îÄ README.md                          # Documenta√ß√£o principal
```

---

## üîÑ Fluxo do Pipeline de Dados IoT

A imagem abaixo representa todo o fluxo do projeto, desde a simula√ß√£o dos dados at√© a an√°lise final dos sensores:

![Fluxo do Pipeline](./img/fluxo.png)

---

## üõ†Ô∏è Arquitetura do Projeto

A imagem abaixo resume a arquitetura geral do projeto `projeto-nosql-iot`, desde a simula√ß√£o dos sensores IoT at√© a visualiza√ß√£o dos dados:

![Arquitetura do Projeto](./img/fluxograma_iot.png)

---

## ‚è∞ Executar o pipeline (via Airflow)

1. Acesse o navegador em `http://localhost:8080`
2. Ative a DAG `pipeline_iot_nosql`
3. Clique no bot√£o ‚ñ∂Ô∏è para rodar manualmente
4. Veja o gr√°fico com as etapas

---

## ‚öñÔ∏è Executar manualmente pelo terminal (sem Airflow)

```bash
# Rodar o pipeline completo com Python:
python src/pipeline.py
```

---

## üîß Rodar os testes

```bash
# Windows
$env:PYTHONPATH="." ; pytest tests/

# Linux/macOS
PYTHONPATH=. pytest tests/
```

---

## üìÖ Integra√ß√£o Cont√≠nua (CI) com GitHub Actions

Este projeto possui uma esteira de CI implementada com GitHub Actions que executa automaticamente:

- Instala√ß√£o de depend√™ncias
- Execu√ß√£o dos scripts ETL (`gerar_dados_json.py`, `extracao.py`, `transformacao.py`, `carga.py`)
- Execu√ß√£o dos testes unit√°rios com `pytest`
- Valida√ß√£o do reposit√≥rio a cada `push` ou `pull request` na branch `main`

> Veja os resultados na aba **Actions** do reposit√≥rio

---

## ‚öñÔ∏è Como o Projeto Foi Constru√≠do (Etapas)

### 1. Planejamento da Arquitetura
- Estrutura baseada em ETL (extra√ß√£o, transforma√ß√£o e carga)
- Apache Airflow como orquestrador, Docker como infraestrutura

### 2. Simula√ß√£o de Dados IoT
- Script `gerar_dados_json.py` simula sensores de bombas e compressores

### 3. Constru√ß√£o do Pipeline
- **Extra√ß√£o** (`extracao.py`): JSON para DataFrame
- **Transforma√ß√£o** (`transformacao.py`): limpeza, tipos, normaliza√ß√£o
- **Carga** (`carga.py`): salva em Parquet no `data/lake/`
- **DAG Airflow**: controla a ordem e depend√™ncias com `dag_pipeline.py`

### 4. Cont√™ners com Docker
- `docker-compose.yml` sobe os servi√ßos: Airflow (webserver, scheduler), PostgreSQL (opcional)

### 5. Testes Automatizados
- `pytest` em `tests/test_transformacao.py`

### 6. An√°lise de KPIs com Jupyter
- `exploracao_inicial.ipynb` com pandas, matplotlib, seaborn

---

## üìä An√°lise de Desempenho dos Sensores IoT

Simula sensores monitorando bombas e compressores em plataformas offshore.
Baseados nos arquivos `.parquet`, analisamos:

- Distribui√ß√£o por tipo de sensor
- M√©dias e desvios
- Evolu√ß√£o temporal
- Anomalias detectadas

Ferramentas usadas:
- `pandas`, `matplotlib`, `seaborn`

Notebook: `notebooks/exploracao_inicial.ipynb`

---

## üì∏ Prints do Funcionamento

### Docker Desktop com os containers ativos:
![docker](./img/docker_containers.png)

### DAG executada com sucesso no Airflow:
![airflow](./img/airflow_dag_pipeline.png)

---

## üìä Poss√≠veis Evolu√ß√µes Futuras

- Integra√ß√£o com MongoDB Atlas real
- Deploy do Data Lake em S3 ou Azure Blob Storage
- Dashboard com Streamlit ou Power BI
- Processamento em tempo real com Apache Kafka
- CD para deploy automatizado com Render ou EC2

---

## üë§ Autor

**Adriano Vilela**  
Engenheiro de Dados em forma√ß√£o | Pythonista em constru√ß√£o  
[LinkedIn](https://linkedin.com/in/adrianogvs) ‚Ä¢ [GitHub](https://github.com/Adrianogvs)

---

Pronto! Agora √© s√≥ apertar o play na DAG e ver a m√°gia acontecer! üöÄ