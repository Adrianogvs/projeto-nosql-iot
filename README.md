# üöÄ Projeto NoSQL IoT - Engenharia de Dados

[![CI](https://github.com/SEU_USUARIO/projeto-nosql-iot/actions/workflows/python-pipeline.yml/badge.svg)](https://github.com/SEU_USUARIO/projeto-nosql-iot/actions)

Este projeto simula um cen√°rio real da Petrobras, onde sensores IoT monitoram bombas e compressores em plataformas offshore. Os dados s√£o enviados como documentos JSON e armazenados em um banco NoSQL (MongoDB). O pipeline trata, transforma e armazena os dados no formato otimizado (Parquet), pronto para an√°lise.

---

## üìÅ Estrutura do Projeto

```bash
projeto-nosql-iot/
‚îú‚îÄ‚îÄ data/                    # Dados simulados e tratados
‚îÇ   ‚îú‚îÄ‚îÄ raw/                # Arquivos JSON gerados (ex: sensores_simulados.json)
‚îÇ   ‚îú‚îÄ‚îÄ processed/          # Arquivos intermedi√°rios em CSV/Parquet
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sensores_extraidos.csv
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sensores_extraidos.parquet
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sensores_transformados.csv
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ sensores_transformados.parquet
‚îÇ   ‚îî‚îÄ‚îÄ lake/               # Resultado final no formato Parquet
‚îÇ       ‚îî‚îÄ‚îÄ sensores_lake.parquet
‚îÇ
‚îú‚îÄ‚îÄ src/                    # C√≥digo principal do pipeline
‚îÇ   ‚îú‚îÄ‚îÄ extracao.py         # L√™ o JSON e transforma em DataFrame
‚îÇ   ‚îú‚îÄ‚îÄ transformacao.py    # Limpa, padroniza e transforma os dados
‚îÇ   ‚îú‚îÄ‚îÄ carga.py            # Salva os dados no formato final
‚îÇ   ‚îî‚îÄ‚îÄ pipeline.py         # Executa todas as etapas em sequ√™ncia
‚îÇ
‚îú‚îÄ‚îÄ scripts/                # Scripts utilit√°rios
‚îÇ   ‚îî‚îÄ‚îÄ gerar_dados_json.py # Gera√ß√£o de dados simulados
‚îÇ
‚îú‚îÄ‚îÄ tests/                  # Testes automatizados
‚îÇ   ‚îî‚îÄ‚îÄ test_transformacao.py # Testa a fun√ß√£o de transforma√ß√£o de dados
‚îÇ
‚îú‚îÄ‚îÄ dags/                   # (Opcional) DAG para execu√ß√£o no Apache Airflow
‚îÇ   ‚îî‚îÄ‚îÄ dag_iot_airflow.py
‚îÇ
‚îú‚îÄ‚îÄ .github/workflows/      # Configura√ß√£o do CI/CD com GitHub Actions
‚îÇ   ‚îî‚îÄ‚îÄ python-pipeline.yml
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt        # Depend√™ncias do projeto
‚îú‚îÄ‚îÄ README.md               # Documenta√ß√£o do projeto
‚îî‚îÄ‚îÄ .env.example            # Exemplo de vari√°veis de ambiente
```

---

## ‚öôÔ∏è Tecnologias Utilizadas

- Python 3.12
- MongoDB (estrutura NoSQL simulada)
- Pandas e PyArrow (Parquet)
- Apache Airflow (orquestra√ß√£o)
- Pytest (testes)
- GitHub Actions (CI/CD)

---

## üß© Etapas do Pipeline - Passo a Passo

### 1. **Gera√ß√£o de Dados Simulados** (`scripts/gerar_dados_json.py`)
Gera um arquivo JSON com dados de sensores simulando leituras de temperatura, press√£o e vibra√ß√£o em plataformas offshore.

```bash
python scripts/gerar_dados_json.py
```

> üìÑ Gera o arquivo `data/raw/sensores_simulados.json`

---

### 2. **Extra√ß√£o dos Dados** (`src/extracao.py`)
Carrega o JSON com as leituras e transforma em um DataFrame Pandas, salvando tamb√©m em CSV e Parquet na pasta `processed`.

```bash
python src/extracao.py
```

> üìÅ Salva: `data/processed/sensores_extraidos.csv` e `.parquet`

---

### 3. **Transforma√ß√£o dos Dados** (`src/transformacao.py`)
Aplica as regras de padroniza√ß√£o e limpeza:
- Remove nulos
- Converte `timestamp` para datetime
- Padroniza nomes dos sensores
- Converte valores para float

```bash
python src/transformacao.py
```

> üìÅ Salva: `data/processed/sensores_transformados.csv` e `.parquet`

---

### 4. **Carga no Data Lake** (`src/carga.py`)
Carrega o arquivo transformado e salva o resultado final no Data Lake local em formato Parquet.

```bash
python src/carga.py
```

> üìÅ Salva: `data/lake/sensores_lake.parquet`

---

### 5. **Pipeline Sequencial** (`src/pipeline.py`)
Executa automaticamente as etapas anteriores em sequ√™ncia.

```bash
python src/pipeline.py
```

---

### 6. **Testes Automatizados** (`tests/test_transformacao.py`)
Testa se a transforma√ß√£o dos dados funciona corretamente, usando dados simulados.

```bash
# Windows
$env:PYTHONPATH="." ; pytest tests/

# Linux/Mac
PYTHONPATH=. pytest tests/
```

---

### 7. **CI/CD com GitHub Actions** (`.github/workflows/python-pipeline.yml`)
Executa automaticamente:
- Instala√ß√£o das depend√™ncias
- Execu√ß√£o do pipeline completo
- Execu√ß√£o dos testes com Pytest

> ‚úÖ Rodado em cada push/pull request na branch `main`

---

## ‚ñ∂Ô∏è Como Executar Localmente

```bash
# Clonar o reposit√≥rio
git clone https://github.com/SEU_USUARIO/projeto-nosql-iot.git
cd projeto-nosql-iot

# Criar ambiente virtual
python -m venv .venv
.venv\Scripts\activate

# Instalar depend√™ncias
pip install -r requirements.txt

# Rodar pipeline completo
python src/pipeline.py
```

---

## ‚úÖ Rodar os testes manualmente

```bash
# No Windows (PowerShell)
$env:PYTHONPATH="." ; pytest tests/

# No Linux/Mac
PYTHONPATH=. pytest tests/
```

---

## üß† Poss√≠veis Evolu√ß√µes

- Conectar com MongoDB Atlas real
- Integra√ß√£o com Apache Kafka
- Dashboard com Streamlit
- Salvar Parquet na nuvem (S3, Azure Blob)
- Deploy do Airflow em Docker

---

## üë®‚Äçüíª Autor

**Adriano V. S.**  
Engenheiro de Dados | Pythonista em Constru√ß√£o  
[LinkedIn](https://linkedin.com/in/SEU_USUARIO) ‚Ä¢ [GitHub](https://github.com/SEU_USUARIO)

